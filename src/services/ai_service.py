"""AI Service ‚Äî GPT-4o primary, Gemini fallback."""
import asyncio
import logging
from config import settings

logger = logging.getLogger(__name__)


class AIService:
    """Service for generating content using AI (GPT-4o primary, Gemini fallback)."""

    def __init__(self):
        # --- OpenAI (primary) ---
        self.openai_client = None
        if settings.openai_api_key:
            try:
                from openai import AsyncOpenAI
                self.openai_client = AsyncOpenAI(api_key=settings.openai_api_key)
                logger.info("OpenAI client initialized (GPT-4o)")
            except Exception as e:
                logger.warning(f"OpenAI init failed: {e}")

        # --- Gemini (fallback) ---
        self.gemini_models = []
        if settings.gemini_api_key:
            try:
                import google.generativeai as genai
                genai.configure(api_key=settings.gemini_api_key)
                self.gemini_models = [
                    genai.GenerativeModel('models/gemini-flash-lite-latest'),
                    genai.GenerativeModel('models/gemini-2.0-flash-lite'),
                ]
                logger.info("Gemini fallback models initialized")
            except Exception as e:
                logger.warning(f"Gemini init failed: {e}")

    async def _call_openai(self, prompt: str, system: str = None, timeout: float = 20.0) -> tuple[str | None, str | None]:
        """Call GPT-4o with timeout and quota error handling. Returns (text, error)."""
        if not self.openai_client:
            return None, "OpenAI client not initialized"
        try:
            messages = []
            if system:
                messages.append({"role": "system", "content": system})
            messages.append({"role": "user", "content": prompt})

            response = await asyncio.wait_for(
                self.openai_client.chat.completions.create(
                    model="gpt-4o",
                    messages=messages,
                    max_tokens=800,
                    temperature=0.7,
                ),
                timeout=timeout
            )
            text = response.choices[0].message.content.strip()
            if text and len(text) > 10:
                return text, None
            return None, "Empty response from OpenAI"
        except asyncio.TimeoutError:
            logger.warning("OpenAI timed out")
            return None, "OpenAI Timeout"
        except Exception as e:
            err = str(e)
            logger.warning(f"OpenAI error: {e}")
            if "429" in err or "quota" in err.lower() or "insufficient_quota" in err:
                return None, "OpenAI Quota Exceeded"
            return None, f"OpenAI Error: {str(e)[:50]}"

    async def _call_gemini(self, prompt: str, timeout: float = 15.0) -> tuple[str | None, str | None]:
        """Call Gemini models with timeout and quota error handling. Returns (text, error)."""
        last_error = "Gemini Call Failed"
        
        for model in self.gemini_models:
            try:
                response = await asyncio.wait_for(
                    model.generate_content_async(prompt), timeout=timeout
                )
                text = response.text.strip() if response and hasattr(response, 'text') else None
                if text and len(text) > 10:
                    return text, None
            except asyncio.TimeoutError:
                logger.warning("Gemini model timed out")
                last_error = "Gemini Timeout"
            except Exception as e:
                err = str(e)
                logger.warning(f"Gemini error: {e}")
                if "429" in err or "quota" in err.lower() or "ResourceExhausted" in err:
                    last_error = "Gemini Quota Exceeded"
                else:
                    last_error = f"Gemini Error: {str(e)[:50]}"
                    
        return None, last_error

    async def generate_professional_description(
        self,
        name: str,
        origin: str,
        roast: str,
        notes: list,
        processing: str
    ) -> tuple[str | None, str | None]:
        """Generate a professional coffee description. GPT-4o ‚Üí Gemini fallback. Returns (text, error)."""

        system = (
            "–¢–∏ ‚Äî –ø—Ä–æ—Ñ–µ—Å—ñ–π–Ω–∏–π —à–µ—Ñ-–±–∞—Ä–∏—Å—Ç–∞ Monkeys Coffee. –ü–∏—à–µ—à –∫–æ—Ä–æ—Ç–∫–æ, "
            "–µ–∫—Å–ø–µ—Ä—Ç–Ω–æ —Ç–∞ –∑—Ä–æ–∑—É–º—ñ–ª–æ —è–∫ –¥–ª—è –Ω–æ–≤–∞—á–∫—ñ–≤, —Ç–∞–∫ —ñ –¥–ª—è —Ñ–∞–Ω–∞—Ç—ñ–≤ –∫–∞–≤–∏. "
            "–¢—ñ–ª—å–∫–∏ —É–∫—Ä–∞—ó–Ω—Å—å–∫–∞. HTML: <b>, <i>."
        )

        notes_str = ", ".join(notes) if isinstance(notes, list) else str(notes)

        prompt = f"""–ù–∞–ø–∏—à–∏ –∫–æ—Ä–æ—Ç–∫–∏–π –ø—Ä–æ—Ñ–µ—Å—ñ–π–Ω–∏–π –æ–ø–∏—Å –∫–∞–≤–∏ ({name}, {origin}, {roast}, {processing}, –Ω–æ—Ç–∏: {notes_str}).
        
–°—Ç—Ä—É–∫—Ç—É—Ä–∞ (HTML):
<b>{name}</b>
<i>[1 —Ä–µ—á–µ–Ω–Ω—è: —Ö–∞—Ä–∞–∫—Ç–µ—Ä –ª–æ—Ç—É]</i>

<b>üìã –ü—Ä–æ—Ñ—ñ–ª—å:</b>
‚Ä¢ <b>–¢—ñ–ª–æ:</b> [1-3 —Å–ª–æ–≤–∞]
‚Ä¢ <b>–°–º–∞–∫:</b> {notes_str}

–ü–∏—à–∏ –±–µ–∑ –≤–æ–¥–∏, –º–∞–∫—Å–∏–º—É–º 30-40 —Å–ª—ñ–≤."""

        # Try GPT-4o first
        result, openai_error = await self._call_openai(prompt, system=system)
        if result:
            logger.info(f"GPT-4o generated description for {name}")
            return result, None

        # Fallback to Gemini
        full_prompt = f"{system}\n\n{prompt}"
        result, gemini_error = await self._call_gemini(full_prompt)
        if result:
            logger.info(f"Gemini generated description for {name}")
            return result, None

        # Return the most relevant error
        return None, openai_error or gemini_error

    async def generate_description_narrative(
        self,
        name: str,
        origin: str,
        roast: str,
        notes: list,
        processing: str
    ) -> tuple[str | None, str | None]:
        """Generate a short punchy narrative. GPT-4o ‚Üí Gemini fallback. Returns (text, error)."""

        system = (
            "–¢–∏ ‚Äî –∑—É—Ö–≤–∞–ª–∏–π –∫–æ–ø—ñ—Ä–∞–π—Ç–µ—Ä Monkeys Coffee Roasters. "
            "–°—Ç–∏–ª—å: –µ–Ω–µ—Ä–≥—ñ–π–Ω–∏–π, –∑ –≥—É–º–æ—Ä–æ–º, –∑ –µ–º–æ–¥–∑—ñ üêí‚òï‚ö°. –ü–∏—à–µ—à —Ç—ñ–ª—å–∫–∏ —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é. "
            "–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π HTML —Ç–µ–≥–∏ <b> —Ç–∞ <i>."
        )

        notes_str = ", ".join(notes) if isinstance(notes, list) else str(notes)

        prompt = f"""–ù–∞–ø–∏—à–∏ –∫–æ—Ä–æ—Ç–∫–∏–π (2-3 —Ä–µ—á–µ–Ω–Ω—è) –≤–∏–±—É—Ö–æ–≤–∏–π –æ–ø–∏—Å –∫–∞–≤–∏. –ë–µ–∑ –ø–æ—Ä–∞–¥, —Ç—ñ–ª—å–∫–∏ —Å–º–∞–∫ —Ç–∞ –µ–º–æ—Ü—ñ—è.

–î–∞–Ω—ñ: {name}, {origin}, –æ–±—Å–º–∞–∂–∫–∞: {roast}, –Ω–æ—Ç–∫–∏: {notes_str}, –æ–±—Ä–æ–±–∫–∞: {processing}

–§–æ—Ä–º–∞—Ç:
üî• <b>{name}</b>. [–ó—É—Ö–≤–∞–ª–∏–π –æ–ø–∏—Å —Å–º–∞–∫—É –∑ –µ–º–æ—Ü—ñ—î—é ‚Äî 2-3 —Ä–µ—á–µ–Ω–Ω—è. –û–±–æ–≤'—è–∑–∫–æ–≤–æ –∑–≥–∞–¥–∞–π –Ω–æ—Ç–∫–∏ —Å–º–∞–∫—É!]"""

        result, openai_error = await self._call_openai(prompt, system=system)
        if result:
            return result, None

        full_prompt = f"{system}\n\n{prompt}"
        result, gemini_error = await self._call_gemini(full_prompt)
        
        if result:
            return result, None
            
        return None, openai_error or gemini_error

    async def generate_smart_editor_text(self, key: str, prompt: str) -> tuple[str | None, str | None]:
        """Generate text for Smart Editor content keys. GPT-4o ‚Üí Gemini fallback. Returns (text, error)."""

        system = (
            "–¢–∏ ‚Äî –ø—Ä–æ—Ñ–µ—Å—ñ–π–Ω–∏–π —Ä–µ–¥–∞–∫—Ç–æ—Ä Monkeys Coffee Roasters. "
            "–¢–≤–æ—è –º–µ—Ç–∞ ‚Äî –ø–∏—Å–∞—Ç–∏ —á—ñ—Ç–∫—ñ, —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–≤–∞–Ω—ñ —Ç–∞ –ø—Ä–æ–¥–∞—é—á—ñ —Ç–µ–∫—Å—Ç–∏ –¥–ª—è Telegram-–±–æ—Ç–∞. "
            "–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π –º–∞—Ä–∫–æ–≤–∞–Ω—ñ —Å–ø–∏—Å–∫–∏, –µ–º–æ–¥–∑—ñ (–ø–æ–º—ñ—Ä–Ω–æ) —Ç–∞ HTML-—Ç–µ–≥–∏ <b> –¥–ª—è –∞–∫—Ü–µ–Ω—Ç—ñ–≤. "
            "–°—Ç–∏–ª—å: –¥—ñ–ª–æ–≤–∏–π, –∞–ª–µ –¥—Ä—É–∂–Ω—ñ–π, –±–µ–∑ –∑–∞–π–≤–æ–≥–æ '—à—É–º—É' —Ç–∞ –≤–æ–¥–∏. "
            "–¢—ñ–ª—å–∫–∏ —É–∫—Ä–∞—ó–Ω—Å—å–∫–∞ –º–æ–≤–∞."
        )

        # Try GPT-4o
        result, error = await self._call_openai(prompt, system=system)
        if result:
            return result, None
            
        # Fallback to Gemini
        full_prompt = f"{system}\n\n{prompt}"
        result, gemini_error = await self._call_gemini(full_prompt)
        
        if result:
            return result, None
            
        # Return the most relevant error (OpenAI if set, else Gemini)
        return None, error or gemini_error


# Singleton instance
ai_service = AIService()
